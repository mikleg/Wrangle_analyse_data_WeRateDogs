{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 0,
        "height": 4,
        "hidden": false,
        "row": 0,
        "width": 4
       },
       "report_default": {
        "hidden": false
       }
      }
     }
    }
   },
   "source": [
    "# Project: Wrangling and Analyze Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Gathering\n",
    "In the cell below, gather **all** three pieces of data for this project and load them in the notebook. **Note:** the methods required to gather each data are different.\n",
    "1. Directly download the WeRateDogs Twitter archive data (twitter_archive_enhanced.csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "hidden": true
       },
       "report_default": {
        "hidden": true
       }
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import urllib.request\n",
    "import pandas as pd\n",
    "import tweepy\n",
    "import os\n",
    "import os.path\n",
    "import requests\n",
    "import json\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/workspace'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cwd = os.getcwd() \n",
    "base_path = os.getcwd()\n",
    "cwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_exists(base_p, file_name):\n",
    "    return(os.path.exists(os.path.join(base_p, file_name)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>in_reply_to_status_id</th>\n",
       "      <th>in_reply_to_user_id</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>source</th>\n",
       "      <th>text</th>\n",
       "      <th>retweeted_status_id</th>\n",
       "      <th>retweeted_status_user_id</th>\n",
       "      <th>retweeted_status_timestamp</th>\n",
       "      <th>expanded_urls</th>\n",
       "      <th>rating_numerator</th>\n",
       "      <th>rating_denominator</th>\n",
       "      <th>name</th>\n",
       "      <th>doggo</th>\n",
       "      <th>floofer</th>\n",
       "      <th>pupper</th>\n",
       "      <th>puppo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>892420643555336193</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2017-08-01 16:23:56 +0000</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/iphone\" r...</td>\n",
       "      <td>This is Phineas. He's a mystical boy. Only eve...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://twitter.com/dog_rates/status/892420643...</td>\n",
       "      <td>13</td>\n",
       "      <td>10</td>\n",
       "      <td>Phineas</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>892177421306343426</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2017-08-01 00:17:27 +0000</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/iphone\" r...</td>\n",
       "      <td>This is Tilly. She's just checking pup on you....</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://twitter.com/dog_rates/status/892177421...</td>\n",
       "      <td>13</td>\n",
       "      <td>10</td>\n",
       "      <td>Tilly</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>891815181378084864</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2017-07-31 00:18:03 +0000</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/iphone\" r...</td>\n",
       "      <td>This is Archie. He is a rare Norwegian Pouncin...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://twitter.com/dog_rates/status/891815181...</td>\n",
       "      <td>12</td>\n",
       "      <td>10</td>\n",
       "      <td>Archie</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0            tweet_id  in_reply_to_status_id  in_reply_to_user_id  \\\n",
       "0           0  892420643555336193                    NaN                  NaN   \n",
       "1           1  892177421306343426                    NaN                  NaN   \n",
       "2           2  891815181378084864                    NaN                  NaN   \n",
       "\n",
       "                   timestamp  \\\n",
       "0  2017-08-01 16:23:56 +0000   \n",
       "1  2017-08-01 00:17:27 +0000   \n",
       "2  2017-07-31 00:18:03 +0000   \n",
       "\n",
       "                                              source  \\\n",
       "0  <a href=\"http://twitter.com/download/iphone\" r...   \n",
       "1  <a href=\"http://twitter.com/download/iphone\" r...   \n",
       "2  <a href=\"http://twitter.com/download/iphone\" r...   \n",
       "\n",
       "                                                text  retweeted_status_id  \\\n",
       "0  This is Phineas. He's a mystical boy. Only eve...                  NaN   \n",
       "1  This is Tilly. She's just checking pup on you....                  NaN   \n",
       "2  This is Archie. He is a rare Norwegian Pouncin...                  NaN   \n",
       "\n",
       "   retweeted_status_user_id retweeted_status_timestamp  \\\n",
       "0                       NaN                        NaN   \n",
       "1                       NaN                        NaN   \n",
       "2                       NaN                        NaN   \n",
       "\n",
       "                                       expanded_urls  rating_numerator  \\\n",
       "0  https://twitter.com/dog_rates/status/892420643...                13   \n",
       "1  https://twitter.com/dog_rates/status/892177421...                13   \n",
       "2  https://twitter.com/dog_rates/status/891815181...                12   \n",
       "\n",
       "   rating_denominator     name doggo floofer pupper puppo  \n",
       "0                  10  Phineas  None    None   None  None  \n",
       "1                  10    Tilly  None    None   None  None  \n",
       "2                  10   Archie  None    None   None  None  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if (not(file_exists(base_path,\"twitter-archive-enhanced.csv\"))):\n",
    "    url = 'https://d17h27t6h515a5.cloudfront.net/topher/2017/August/59a4e958_twitter-archive-enhanced/twitter-archive-enhanced.csv'\n",
    "    file_to_read = urllib.request.urlopen(url)\n",
    "    df_ta = pd.read_csv(file_to_read)\n",
    "    df_ta.to_csv(tw_arch_file)\n",
    "else:\n",
    "    file_to_read = os.path.join(base_path, \"twitter-archive-enhanced.csv\")\n",
    "    df_ta = pd.read_csv(file_to_read)\n",
    "df_ta.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Use the Requests library to download the tweet image prediction (image_predictions.tsv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>jpg_url</th>\n",
       "      <th>img_num</th>\n",
       "      <th>p1</th>\n",
       "      <th>p1_conf</th>\n",
       "      <th>p1_dog</th>\n",
       "      <th>p2</th>\n",
       "      <th>p2_conf</th>\n",
       "      <th>p2_dog</th>\n",
       "      <th>p3</th>\n",
       "      <th>p3_conf</th>\n",
       "      <th>p3_dog</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>666020888022790149</td>\n",
       "      <td>https://pbs.twimg.com/media/CT4udn0WwAA0aMy.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>Welsh_springer_spaniel</td>\n",
       "      <td>0.465074</td>\n",
       "      <td>True</td>\n",
       "      <td>collie</td>\n",
       "      <td>0.156665</td>\n",
       "      <td>True</td>\n",
       "      <td>Shetland_sheepdog</td>\n",
       "      <td>0.061428</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>666029285002620928</td>\n",
       "      <td>https://pbs.twimg.com/media/CT42GRgUYAA5iDo.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>redbone</td>\n",
       "      <td>0.506826</td>\n",
       "      <td>True</td>\n",
       "      <td>miniature_pinscher</td>\n",
       "      <td>0.074192</td>\n",
       "      <td>True</td>\n",
       "      <td>Rhodesian_ridgeback</td>\n",
       "      <td>0.072010</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>666033412701032449</td>\n",
       "      <td>https://pbs.twimg.com/media/CT4521TWwAEvMyu.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>German_shepherd</td>\n",
       "      <td>0.596461</td>\n",
       "      <td>True</td>\n",
       "      <td>malinois</td>\n",
       "      <td>0.138584</td>\n",
       "      <td>True</td>\n",
       "      <td>bloodhound</td>\n",
       "      <td>0.116197</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             tweet_id                                          jpg_url  \\\n",
       "0  666020888022790149  https://pbs.twimg.com/media/CT4udn0WwAA0aMy.jpg   \n",
       "1  666029285002620928  https://pbs.twimg.com/media/CT42GRgUYAA5iDo.jpg   \n",
       "2  666033412701032449  https://pbs.twimg.com/media/CT4521TWwAEvMyu.jpg   \n",
       "\n",
       "   img_num                      p1   p1_conf  p1_dog                  p2  \\\n",
       "0        1  Welsh_springer_spaniel  0.465074    True              collie   \n",
       "1        1                 redbone  0.506826    True  miniature_pinscher   \n",
       "2        1         German_shepherd  0.596461    True            malinois   \n",
       "\n",
       "    p2_conf  p2_dog                   p3   p3_conf  p3_dog  \n",
       "0  0.156665    True    Shetland_sheepdog  0.061428    True  \n",
       "1  0.074192    True  Rhodesian_ridgeback  0.072010    True  \n",
       "2  0.138584    True           bloodhound  0.116197    True  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = 'https://d17h27t6h515a5.cloudfront.net/topher/2017/August/599fd2ad_image-predictions/image-predictions.tsv'\n",
    "response = urllib.request.urlopen(url)\n",
    "df_ipr = pd.read_csv(response, sep = '\\t')\n",
    "df_ipr.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get all tweet_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2356"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_ids = df_ta['tweet_id'].astype(str)\n",
    "tweet_ids.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Use the Tweepy library to query additional data via the Twitter API (tweet_json.txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_bearer_token = os.path.join(base_path, \"bearer_token.txt\")\n",
    "with open(path_to_bearer_token, 'r') as f:    \n",
    "    bearer_token = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tweat_url(tw_id):\n",
    "    twitter_url = \"https://api.twitter.com/2/tweets/\" + str(tw_id)\n",
    "    query_params = {#'id': tw_id,\n",
    "                    'expansions': 'author_id,in_reply_to_user_id,geo.place_id',\n",
    "                    'tweet.fields': 'id,author_id,in_reply_to_user_id,geo,conversation_id,created_at,lang,public_metrics,referenced_tweets,reply_settings,source,text',\n",
    "                    'user.fields': 'id,name,username,created_at,description,public_metrics,verified',\n",
    "                    'place.fields': 'full_name,id,country,country_code,geo,name,place_type',\n",
    "                    'next_token': {}}\n",
    "    return (twitter_url, query_params)\n",
    "\n",
    "def get_tweats_url(tw_ids):\n",
    "    twitter_url = \"https://api.twitter.com/2/tweets\"\n",
    "    query_params = {'ids': tw_ids,\n",
    "                    'expansions': 'author_id,in_reply_to_user_id,geo.place_id',\n",
    "                    'tweet.fields': 'id,author_id,in_reply_to_user_id,geo,conversation_id,created_at,lang,public_metrics,referenced_tweets,reply_settings,source,text',\n",
    "                    'user.fields': 'id,name,username,created_at,description,public_metrics,verified',\n",
    "                    'place.fields': 'full_name,id,country,country_code,geo,name,place_type',\n",
    "                    'next_token': {}}\n",
    "    return (twitter_url, query_params)\n",
    "\n",
    "def connect_to_twitter(url, headers, params, next_token = None):\n",
    "    params['next_token'] = next_token\n",
    "    resp = requests.request(\"GET\", url, headers = headers, params = params)\n",
    "    print(\"Endpoint Response Code: \" + str(resp.status_code))\n",
    "    if resp.status_code != 200:\n",
    "        raise Exception(resp.status_code, resp.text)\n",
    "    return (resp.json())\n",
    "  \n",
    "    \n",
    "def get_ids_string(start, quantity, tw_ids_series): #format sting with ids for pulling\n",
    "    result = ''\n",
    "    for i in range(start, start + quantity):\n",
    "        if i in tw_ids_series.index:\n",
    "            if len(result) > 0:\n",
    "                result = result +','+ tw_ids_series[i]\n",
    "            else:\n",
    "                result = tw_ids_series[i]\n",
    "        else:\n",
    "            i-=1\n",
    "            break\n",
    "    return (result, i-start+1)\n",
    "def add_timestamp(my_series):\n",
    "    print('type_my_ser:', type(my_series) )\n",
    "    cur_time = time.time()\n",
    "    with open(os.path.join(base_path, 'rqsts.csv'),'a') as file:\n",
    "        file.write(str(cur_time))\n",
    "        file.write('\\n')\n",
    "    my_series = my_series.values.tolist()\n",
    "    my_series.append(cur_time)\n",
    "    my_series = pd.Series(my_series)\n",
    "    return(my_series)\n",
    "    \n",
    "def make_request(tweets_ids, time_window = 1700, requests_limit = 50):\n",
    "    if (file_exists(base_path, 'rqsts.csv')):\n",
    "        rqsts = pd.read_csv(os.path.join(base_path, 'rqsts.csv'), names = ['timestamp'])\n",
    "        rqsts_s = rqsts['timestamp']\n",
    "    else:\n",
    "        rqsts_s = pd.Series([])\n",
    "    current_time = time.time()\n",
    "    rqsts_f = rqsts_s[rqsts_s > (current_time - time_window)]\n",
    "    if (rqsts_f.shape[0] < requests_limit):\n",
    "        #do request\n",
    "        url = get_tweats_url(tweets_ids)\n",
    "        headers = {\"Authorization\": \"Bearer \" + bearer_token}\n",
    "        rqsts_f = rqsts_f.values.tolist()\n",
    "        rqsts_f.append(current_time)\n",
    "        rqsts_f = pd.Series(rqsts_f)\n",
    "        rqsts_f.to_csv(os.path.join(base_path, 'rqsts.csv'))\n",
    "        json_resp = connect_to_twitter(url[0], headers, url[1])\n",
    "        print(\"request completed\")\n",
    "        return(json_resp, True)\n",
    "    else:\n",
    "        print('limit of request exceeded: requests= ', rqsts_f.shape[0], 'limit=', requests_limit)\n",
    "        return(0, False)\n",
    "    \n",
    "#read_JSON RABOTAET NEPRVILNO https://github.com/pandas-dev/pandas/issues/20608# (pri chtenii)\n",
    "def pull_all_data(ids_series, batch_size = 5, time_window = 950, req_limit = 50, sleep_time = 10):\n",
    "    if (file_exists(base_path, 'tweet_json.txt')):\n",
    "        #workaround the insidious bug in read_json method of pandas: \"https://github.com/pandas-dev/pandas/issues/20608#\"\n",
    "        #some random id's in my dataframe became  a little bit different tnan they should be...\n",
    "        #I spend one hour to understand that my code is fine, it's pandas bug :-)\n",
    "        tweet_json_df = pd.read_json(os.path.join(base_path, 'tweet_json.txt'), lines=True, orient='index', convert_axes=False, dtype={}).transpose()\n",
    "        last_id = tweet_json_df.iloc[-1:]['id'].values[0] # the id of the last tweet in the output file (single value)\n",
    "        idx = (df_ta.index[df_ta['tweet_id'].astype(str) == last_id])\n",
    "        start_line = (idx[0].item()) #number of string to start, converted to basic python type\n",
    "    else:\n",
    "        start_line = 0\n",
    "    count = 0\n",
    "    for current_line in range(start_line, ids_series.shape[0], batch_size):\n",
    "        ids = get_ids_string(current_line, batch_size, ids_series)\n",
    "        rsp = ('', False)\n",
    "        while (rsp[1]!= True):\n",
    "            time.sleep(1)\n",
    "            rsp = make_request(ids[0],time_window, req_limit)\n",
    "            if (rsp[1]!= True): #we exceed the limit\n",
    "                print('waiting ', sleep_time, ' seconds' )\n",
    "                time.sleep(sleep_time)\n",
    "        if (not((rsp[0].get('data')) is None)):\n",
    "            missed_id_flag = False\n",
    "            for object_number in range(0,len(rsp[0].get('data'))): #check, maybe ids[1] +1\n",
    "                if (len(rsp[0].get('data')) < batch_size):\n",
    "                    missed_id_flag = True\n",
    "                with open('tweet_json.txt', 'a') as out_f:\n",
    "                    json.dump(rsp[0].get('data')[object_number],out_f)\n",
    "                    out_f.write('\\n')\n",
    "                    count +=1\n",
    "            if (missed_id_flag):\n",
    "                print('Missed id is here ------------>')\n",
    "    print('records completed =', count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('892420643555336193,892177421306343426,891815181378084864,891689557279858688,891327558926688256,891087950875897856,890971913173991426,890729181411237888,890609185150312448,890240255349198849,890006608113172480,889880896479866881,889665388333682689,889638837579907072,889531135344209921,889278841981685760,888917238123831296,888804989199671297,888554962724278272,888202515573088257', 20)\n",
      "Endpoint Response Code: 200\n",
      "request completed\n",
      "Missed id is here ------------>\n",
      "('888078434458587136,887705289381826560,887517139158093824,887473957103951883,887343217045368832,887101392804085760,886983233522544640,886736880519319552,886680336477933568,886366144734445568,886267009285017600,886258384151887873,886054160059072513,885984800019947520,885528943205470208,885518971528720385,885311592912609280,885167619883638784,884925521741709313,884876753390489601', 20)\n",
      "Endpoint Response Code: 200\n",
      "request completed\n",
      "('884562892145688576,884441805382717440,884247878851493888,884162670584377345,883838122936631299,883482846933004288,883360690899218434,883117836046086144,882992080364220416,882762694511734784,882627270321602560,882268110199369728,882045870035918850,881906580714921986,881666595344535552,881633300179243008,881536004380872706,881268444196462592,880935762899988482,880872448815771648', 20)\n",
      "Endpoint Response Code: 200\n",
      "request completed\n",
      "('880465832366813184,880221127280381952,880095782870896641,879862464715927552,879674319642796034,879492040517615616,879415818425184262,879376492567855104,879130579576475649,879050749262655488,879008229531029506,878776093423087618,878604707211726852,878404777348136964,878316110768087041,878281511006478336,878057613040115712,877736472329191424,877611172832227328,877556246731214848', 20)\n",
      "Endpoint Response Code: 200\n",
      "request completed\n",
      "('877316821321428993,877201837425926144,876838120628539392,876537666061221889,876484053909872640,876120275196170240,875747767867523072,875144289856114688,875097192612077568,875021211251597312,874680097055178752,874434818259525634,874296783580663808,874057562936811520,874012996292530176,873697596434513921,873580283840344065,873337748698140672,873213775632977920,872967104147763200', 20)\n",
      "Endpoint Response Code: 200\n",
      "request completed\n",
      "Missed id is here ------------>\n",
      "('872820683541237760,872668790621863937,872620804844003328,872486979161796608,872261713294495745,872122724285648897,871879754684805121,871762521631449091,871515927908634625,871166179821445120,871102520638267392,871032628920680449,870804317367881728,870726314365509632,870656317836468226,870374049280663552,870308999962521604,870063196459192321,869988702071779329,869772420881756160', 20)\n",
      "Endpoint Response Code: 200\n",
      "request completed\n",
      "Missed id is here ------------>\n",
      "('869702957897576449,869596645499047938,869227993411051520,868880397819494401,868639477480148993,868622495443632128,868552278524837888,867900495410671616,867774946302451713,867421006826221569,867072653475098625,867051520902168576,866816280283807744,866720684873056260,866686824827068416,866450705531457537,866334964761202691,866094527597207552,865718153858494464,865359393868664832', 20)\n",
      "Endpoint Response Code: 200\n",
      "request completed\n",
      "Missed id is here ------------>\n",
      "('865006731092295680,864873206498414592,864279568663928832,864197398364647424,863907417377173506,863553081350529029,863471782782697472,863432100342583297,863427515083354112,863079547188785154,863062471531167744,862831371563274240,862722525377298433,862457590147678208,862096992088072192,861769973181624320,861383897657036800,861288531465048066,861005113778896900,860981674716409858', 20)\n",
      "Endpoint Response Code: 200\n",
      "request completed\n",
      "Missed id is here ------------>\n",
      "('860924035999428608,860563773140209665,860524505164394496,860276583193509888,860184849394610176,860177593139703809,859924526012018688,859851578198683649,859607811541651456,859196978902773760,859074603037188101,858860390427611136,858843525470990336,858471635011153920,858107933456039936,857989990357356544,857746408056729600,857393404942143489,857263160327368704,857214891891077121', 20)\n",
      "Endpoint Response Code: 200\n",
      "request completed\n",
      "('857062103051644929,857029823797047296,856602993587888130,856543823941562368,856526610513747968,856330835276025856,856288084350160898,856282028240666624,855862651834028034,855860136149123072,855857698524602368,855851453814013952,855818117272018944,855459453768019968,855245323840757760,855138241867124737,854732716440526848,854482394044301312,854365224396361728,854120357044912130', 20)\n",
      "Endpoint Response Code: 200\n",
      "request completed\n",
      "Missed id is here ------------>\n",
      "records completed = 191\n"
     ]
    }
   ],
   "source": [
    "pull_all_data(df_ta['tweet_id'].astype(str).head(200), batch_size = 20, time_window = 905, req_limit = 50, sleep_time = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "888202515573088257,873697596434513921,872668790621863937,872261713294495745,869988702071779329,866816280283807744,861769973181624320,856602993587888130,856330835276025856\n"
     ]
    }
   ],
   "source": [
    "# find all missed id's and try to get it again\n",
    "tweet_json_df = pd.read_json(os.path.join(base_path, 'tweet_json.txt'), lines=True, orient='index', convert_axes=False, dtype={}).transpose()\n",
    "df2 = tweet_json_df[['id','conversation_id']]\n",
    "df1 = df_ta[['tweet_id','timestamp']].head(200)\n",
    "df1['id'] = df_ta['tweet_id'].astype(str)\n",
    "df = pd.merge(df1, df2, on ='id', how =\"left\")[pd.merge(df1, df2, on ='id', how =\"left\")['conversation_id'].isna()]\n",
    "missed_ids = df['tweet_id'].astype(str).str.cat(sep=',')\n",
    "print(missed_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Endpoint Response Code: 200\n",
      "request completed\n",
      "data = null\n"
     ]
    }
   ],
   "source": [
    "rsp = make_request(missed_ids,905, 50)\n",
    "if (not((rsp[0].get('data')) is None)):\n",
    "    for object_number in range(0,len(rsp[0].get('data'))): \n",
    "        if (len(rsp[0].get('data')) < 9):\n",
    "            print('HERE------------>')\n",
    "        with open('bbb.txt', 'a') as out_f:\n",
    "            json.dump(rsp[0].get('data')[object_number],out_f)\n",
    "            out_f.write('\\n')\n",
    "else:\n",
    "    print('data = null')\n",
    "               "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 4,
        "height": 4,
        "hidden": false,
        "row": 28,
        "width": 4
       },
       "report_default": {
        "hidden": false
       }
      }
     }
    }
   },
   "source": [
    "## Assessing Data\n",
    "In this section, detect and document at least **eight (8) quality issues and two (2) tidiness issue**. You must use **both** visual assessment\n",
    "programmatic assessement to assess the data.\n",
    "\n",
    "**Note:** pay attention to the following key points when you access the data.\n",
    "\n",
    "* You only want original ratings (no retweets) that have images. Though there are 5000+ tweets in the dataset, not all are dog ratings and some are retweets.\n",
    "* Assessing and cleaning the entire dataset completely would require a lot of time, and is not necessary to practice and demonstrate your skills in data wrangling. Therefore, the requirements of this project are only to assess and clean at least 8 quality issues and at least 2 tidiness issues in this dataset.\n",
    "* The fact that the rating numerators are greater than the denominators does not need to be cleaned. This [unique rating system](http://knowyourmeme.com/memes/theyre-good-dogs-brent) is a big part of the popularity of WeRateDogs.\n",
    "* You do not need to gather the tweets beyond August 1st, 2017. You can, but note that you won't be able to gather the image predictions for these tweets since you don't have access to the algorithm used.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quality issues\n",
    "1.\n",
    "\n",
    "2.\n",
    "\n",
    "3.\n",
    "\n",
    "4.\n",
    "\n",
    "5.\n",
    "\n",
    "6.\n",
    "\n",
    "7.\n",
    "\n",
    "8."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 0,
        "height": 7,
        "hidden": false,
        "row": 40,
        "width": 12
       },
       "report_default": {
        "hidden": false
       }
      }
     }
    }
   },
   "source": [
    "### Tidiness issues\n",
    "1.\n",
    "\n",
    "2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 4,
        "height": 4,
        "hidden": false,
        "row": 32,
        "width": 4
       },
       "report_default": {
        "hidden": false
       }
      }
     }
    }
   },
   "source": [
    "## Cleaning Data\n",
    "In this section, clean **all** of the issues you documented while assessing. \n",
    "\n",
    "**Note:** Make a copy of the original data before cleaning. Cleaning includes merging individual pieces of data according to the rules of [tidy data](https://cran.r-project.org/web/packages/tidyr/vignettes/tidy-data.html). The result should be a high-quality and tidy master pandas DataFrame (or DataFrames, if appropriate)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make copies of original pieces of data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Issue #1:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Issue #2:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "hidden": true
       },
       "report_default": {
        "hidden": true
       }
      }
     }
    }
   },
   "source": [
    "#### Define"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Storing Data\n",
    "Save gathered, assessed, and cleaned master dataset to a CSV file named \"twitter_archive_master.csv\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyzing and Visualizing Data\n",
    "In this section, analyze and visualize your wrangled data. You must produce at least **three (3) insights and one (1) visualization.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Insights:\n",
    "1.\n",
    "\n",
    "2.\n",
    "\n",
    "3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "extensions": {
   "jupyter_dashboards": {
    "activeView": "report_default",
    "version": 1,
    "views": {
     "grid_default": {
      "cellMargin": 10,
      "defaultCellHeight": 20,
      "maxColumns": 12,
      "name": "grid",
      "type": "grid"
     },
     "report_default": {
      "name": "report",
      "type": "report"
     }
    }
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
